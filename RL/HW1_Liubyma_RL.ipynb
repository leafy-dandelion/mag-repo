{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPU5zhG3n+ooyqbItDA/Olt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leafy-dandelion/mag-repo/blob/main/RL/HW1_Liubyma_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HW1 programming part\n",
        "\n",
        "(a) (coding) Read through vi_and_pi.py and implement policy_evaluation, policy_improvement\n",
        "and policy_iteration. The stopping tolerance (defined as maxs |Vold(s) − Vnew(s)|) is tol =\n",
        "10−3\n",
        ". Use γ = 0.9. Return the optimal value function and the optimal policy. [10pts]\n",
        "\n",
        "(b) (coding) Implement value_iteration in vi_and_pi.py. The stopping tolerance is tol =\n",
        "10−3\n",
        ". Use γ = 0.9. Return the optimal value function and the optimal policy. [10 pts]\n",
        "\n",
        "(c) (written) Run both methods on the Deterministic-4x4-FrozenLake-v0 and\n",
        "Stochastic-4x4-FrozenLake-v0 environments. In the second environment, the dynamics of the\n",
        "world are stochastic. How does stochasticity affect the number of iterations required, and the\n",
        "resulting policy? [5 pts]"
      ],
      "metadata": {
        "id": "Bbp4rm6-R2Mm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## important set up"
      ],
      "metadata": {
        "id": "4eMNbzi3SDnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym==0.10.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymkPM_OvZNbM",
        "outputId": "e8a99364-bca5-45e7-9043-251fb90f3248"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym==0.10.9 in /usr/local/lib/python3.9/dist-packages (0.10.9)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.9/dist-packages (from gym==0.10.9) (2.25.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from gym==0.10.9) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.9/dist-packages (from gym==0.10.9) (1.22.4)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym==0.10.9) (2.0.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from gym==0.10.9) (1.15.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.0->gym==0.10.9) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.0->gym==0.10.9) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.0->gym==0.10.9) (1.26.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.0->gym==0.10.9) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title lake_envs.py\n",
        "# coding: utf-8\n",
        "\"\"\"Defines some frozen lake maps.\"\"\"\n",
        "import gym\n",
        "from gym.envs.toy_text import frozen_lake, discrete\n",
        "from gym.envs.registration import register\n",
        "\n",
        "# De-register environments if there is a collision\n",
        "env_dict = gym.envs.registration.registry.env_specs.copy()\n",
        "for env in env_dict:\n",
        "\tif 'Deterministic-4x4-FrozenLake-v0' in env:\n",
        "\t\tdel gym.envs.registration.registry.env_specs[env]\n",
        "\n",
        "\telif 'Deterministic-8x8-FrozenLake-v0' in env:\n",
        "\t\tdel gym.envs.registration.registry.env_specs[env]\n",
        "\n",
        "\telif 'Stochastic-4x4-FrozenLake-v0' in env:\n",
        "\t\tdel gym.envs.registration.registry.env_specs[env]\n",
        "\n",
        "\n",
        "register(\n",
        "    id='Deterministic-4x4-FrozenLake-v0',\n",
        "    entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv',\n",
        "    kwargs={'map_name': '4x4',\n",
        "            'is_slippery': False})\n",
        "\n",
        "register(\n",
        "    id='Deterministic-8x8-FrozenLake-v0',\n",
        "    entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv',\n",
        "    kwargs={'map_name': '8x8',\n",
        "            'is_slippery': False})\n",
        "\n",
        "register(\n",
        "    id='Stochastic-4x4-FrozenLake-v0',\n",
        "    entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv',\n",
        "    kwargs={'map_name': '4x4',\n",
        "            'is_slippery': True})"
      ],
      "metadata": {
        "cellView": "form",
        "id": "EcqRd_QTRGe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## task (a) and (b)"
      ],
      "metadata": {
        "id": "lAQ1lBMFSMWg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "copied from vi_and_pi.py"
      ],
      "metadata": {
        "id": "SMp_mMyOhlQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "import gym\n",
        "import time\n",
        "#next line is not needed as we copied the program\n",
        "# from lake_envs import *"
      ],
      "metadata": {
        "id": "xWCeeaaEgnO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### MDP Value Iteration and Policy Iteration\n",
        "import argparse\n",
        "import numpy as np\n",
        "import gym\n",
        "import time\n",
        "#from lake_envs import *\n",
        "\n",
        "np.set_printoptions(precision=3)\n",
        "\n",
        "parser = argparse.ArgumentParser(description='A program to run assignment 1 implementations.',\n",
        "\t\t\t\t\t\t\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "\n",
        "parser.add_argument(\"--env\", \n",
        "\t\t\t\t\thelp=\"The name of the environment to run your algorithm on.\", \n",
        "\t\t\t\t\tchoices=[\"Deterministic-4x4-FrozenLake-v0\",\"Stochastic-4x4-FrozenLake-v0\"],\n",
        "\t\t\t\t\tdefault=\"Deterministic-4x4-FrozenLake-v0\")\n",
        "\n",
        "\"\"\"\n",
        "For policy_evaluation, policy_improvement, policy_iteration and value_iteration,\n",
        "the parameters P, nS, nA, gamma are defined as follows:\n",
        "\n",
        "\tP: nested dictionary\n",
        "\t\tFrom gym.core.Environment\n",
        "\t\tFor each pair of states in [1, nS] and actions in [1, nA], P[state][action] is a\n",
        "\t\ttuple of the form (probability, nextstate, reward, terminal) where\n",
        "\t\t\t- probability: float\n",
        "\t\t\t\tthe probability of transitioning from \"state\" to \"nextstate\" with \"action\"\n",
        "\t\t\t- nextstate: int\n",
        "\t\t\t\tdenotes the state we transition to (in range [0, nS - 1])\n",
        "\t\t\t- reward: int\n",
        "\t\t\t\teither 0 or 1, the reward for transitioning from \"state\" to\n",
        "\t\t\t\t\"nextstate\" with \"action\"\n",
        "\t\t\t- terminal: bool\n",
        "\t\t\t  True when \"nextstate\" is a terminal state (hole or goal), False otherwise\n",
        "\tnS: int\n",
        "\t\tnumber of states in the environment\n",
        "\tnA: int\n",
        "\t\tnumber of actions in the environment\n",
        "\tgamma: float\n",
        "\t\tDiscount factor. Number in range [0, 1)\n",
        "\"\"\"\n",
        "\n",
        "def policy_evaluation(P, nS, nA, policy, gamma=0.9, tol=1e-3):\n",
        "  \"\"\"Evaluate the value function from a given policy.\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\tP, nS, nA, gamma:\n",
        "\t\tdefined at beginning of file\n",
        "\tpolicy: np.array[nS]\n",
        "\t\tThe policy to evaluate. Maps states to actions.\n",
        "\ttol: float\n",
        "\t\tTerminate policy evaluation when\n",
        "\t\t\tmax |value_function(s) - prev_value_function(s)| < tol\n",
        "\tReturns\n",
        "\t-------\n",
        "\tvalue_function: np.ndarray[nS]\n",
        "\t\tThe value function of the given policy, where value_function[s] is\n",
        "\t\tthe value of state s\n",
        "    \"\"\"\n",
        "  iter = 3000 #set maximum of iteration\n",
        "  value_function = np.zeros(nS)\n",
        "  \n",
        " \n",
        "  i = 1\n",
        "  d = tol+1\n",
        "  while d > tol and i <= iter:\n",
        "    value_function_new = np.zeros(nS)\n",
        "    for s in range(nS):\n",
        "      # for a in range(nA):\n",
        "\t\t\t\t# P[s][a][0] - transition probability\n",
        "\t\t\t\t# P[s][a][1] - next state\n",
        "\t\t\t\t# P[s][a][2] - reward\n",
        "\t\t\t\t# P[s][a][3] - 1 if next state is target 0 otherwise\n",
        "      a = policy[s]\n",
        "      for p, s_, r, t in P[s][a]:\n",
        "        # if t: #if target                   \n",
        "        #   value_function_new[s] += policy[s]*p*r #no next \n",
        "        # else:\n",
        "        if not t:\n",
        "          value_function_new[s] += r+gamma*p*value_function[int(s_)] #plus next state\n",
        "        else:\n",
        "          value_function_new[s] += r\n",
        "    d = max(abs(value_function-value_function_new))\n",
        "    i+=1\n",
        "    value_function = value_function_new\n",
        "  return value_function\n",
        "\n",
        "\n",
        "def policy_improvement(P, nS, nA, value_from_policy, policy, gamma=0.9):\n",
        "  \"\"\"Given the value function from policy improve the policy.\n",
        "  Parameters\n",
        "  ----------\n",
        "  P, nS, nA, gamma:\n",
        "    defined at beginning of file\n",
        "  value_from_policy: np.ndarray\n",
        "    The value calculated from the policy\n",
        "  policy: np.array\n",
        "    The previous policy.\n",
        "  Returns\n",
        "  -------\n",
        "  new_policy: np.ndarray[nS]\n",
        "    An array of integers. Each integer is the optimal action to take\n",
        "    in that state according to the environment dynamics and the\n",
        "    given value function.\n",
        "  \"\"\"\n",
        "\n",
        "  new_policy = np.zeros(nS, dtype='int')\n",
        " \n",
        "  for s in range(nS):\n",
        "    q_a = np.zeros(nA)\n",
        "    for a in range(nA):\n",
        "      for p, s_, r, t in P[s][a]:\n",
        "        if not t:\n",
        "          q_a[a] += r+gamma*p*value_from_policy[int(s_)]\n",
        "        else:\n",
        "          q_a[a]+=r\n",
        "    new_policy[s] = np.argmax(q_a)\n",
        "  return new_policy\n",
        "\n",
        "\n",
        "def policy_iteration(P, nS, nA, gamma=0.9, tol=10e-3):\n",
        "  \"\"\"Runs policy iteration.\n",
        "\n",
        "\tYou should call the policy_evaluation() and policy_improvement() methods to\n",
        "\timplement this method.\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\tP, nS, nA, gamma:\n",
        "\t\tdefined at beginning of file\n",
        "\ttol: float\n",
        "\t\ttol parameter used in policy_evaluation()\n",
        "\tReturns:\n",
        "\t----------\n",
        "\tvalue_function: np.ndarray[nS]\n",
        "\tpolicy: np.ndarray[nS]\n",
        "  \"\"\"\n",
        "\n",
        "  value_function = np.zeros(nS)\n",
        "  #policy = np.zeros(nS, dtype=int)\n",
        "  i = 0\n",
        "  d = 1\n",
        "  policy = np.random.randint(nA, size=nS)\n",
        "  while i==0 or d > 0 :\n",
        "    V = policy_evaluation(P, nS, nA, policy, gamma, tol)\n",
        "    policy_improve = policy_improvement(P, nS, nA, V, policy, gamma)\n",
        "    i+=1\n",
        "    d = np.linalg.norm(policy - policy_improve, ord=1)\n",
        "    policy = policy_improve \n",
        "  return value_function, policy\n",
        "\n",
        "def value_iteration(P, nS, nA, gamma=0.9, tol=1e-3):\n",
        "  \"\"\"\n",
        "\tLearn value function and policy by using value iteration method for a given\n",
        "\tgamma and environment.\n",
        "\n",
        "\tParameters:\n",
        "\t----------\n",
        "\tP, nS, nA, gamma:\n",
        "\t\tdefined at beginning of file\n",
        "\ttol: float\n",
        "\t\tTerminate value iteration when\n",
        "\t\t\tmax |value_function(s) - prev_value_function(s)| < tol\n",
        "\tReturns:\n",
        "\t----------\n",
        "\tvalue_function: np.ndarray[nS]\n",
        "\tpolicy: np.ndarray[nS]\n",
        "  \"\"\"\n",
        "\n",
        "  value_function = np.zeros(nS)\n",
        "  policy = np.zeros(nS, dtype=int)\n",
        "  d = tol+1\n",
        "  while d>tol:\n",
        "    value_function_new = np.zeros(nS)\n",
        "    for s in range(nS):\n",
        "      op_a = np.zeros(nA)\n",
        "      for a in range(nA): \n",
        "         for p, s_, r, t in P[s][a]:\n",
        "           if not t:\n",
        "             op_a[a] += r+gamma*p*value_function[int(s_)]\n",
        "           else:\n",
        "             op_a[a] += r\n",
        "      value_function_new[s] = np.max(op_a)\n",
        "      policy[s] = np.argmax(op_a)\n",
        "    d=max(abs(value_function_new-value_function))\n",
        "    value_function = value_function_new\n",
        "  return value_function, policy\n",
        "\n",
        "def render_single(env, policy, max_steps=100):\n",
        "  \"\"\"\n",
        "    This function does not need to be modified\n",
        "    Renders policy once on environment. Watch your agent play!\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    env: gym.core.Environment\n",
        "      Environment to play on. Must have nS, nA, and P as\n",
        "      attributes.\n",
        "    Policy: np.array of shape [env.nS]\n",
        "      The action to take at a given state\n",
        "  \"\"\"\n",
        "\n",
        "  episode_reward = 0\n",
        "  ob = env.reset()\n",
        "  for t in range(max_steps):\n",
        "    env.render()\n",
        "    time.sleep(0.25)\n",
        "    a = policy[ob]\n",
        "    ob, rew, done, _ = env.step(a)\n",
        "    episode_reward += rew\n",
        "    if done:\n",
        "      break\n",
        "  env.render();\n",
        "  if not done:\n",
        "    print(\"The agent didn't reach a terminal state in {} steps.\".format(max_steps))\n",
        "  else:\n",
        "  \tprint(\"Episode reward: %f\" % episode_reward)"
      ],
      "metadata": {
        "id": "Yyg6_q7pEKVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task (c): run for 2 different environments"
      ],
      "metadata": {
        "id": "x7AFzAMHRuYt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deterministic"
      ],
      "metadata": {
        "id": "HeiyYDiASXDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Edit below to run policy and value iteration on different environments and\n",
        "# visualize the resulting policies in action!\n",
        "# You may change the parameters in the functions below\n",
        "#args = parser.parse_args(\"Deterministic-4x4-FrozenLake-v0\")\n",
        "\t\n",
        "\t# Make gym environment\n",
        "env = gym.make(\"Deterministic-4x4-FrozenLake-v0\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*25 + \"\\nBeginning Policy Iteration\\n\" + \"-\"*25)\n",
        "\n",
        "V_pi, p_pi = policy_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)\n",
        "render_single(env, p_pi, 100)\n",
        "print(\"\\n\" + \"-\"*25 + \"\\nBeginning Value Iteration\\n\" + \"-\"*25)\n",
        "\n",
        "V_vi, p_vi = value_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)\n",
        "render_single(env, p_vi, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byN08Oo22z8u",
        "outputId": "5c4e3e60-1da8-49c7-d544-1cbd7f6d783c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-------------------------\n",
            "Beginning Policy Iteration\n",
            "-------------------------\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Episode reward: 1.000000\n",
            "\n",
            "-------------------------\n",
            "Beginning Value Iteration\n",
            "-------------------------\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Episode reward: 1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stochastic"
      ],
      "metadata": {
        "id": "UtH_gDajScYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"Stochastic-4x4-FrozenLake-v0\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*25 + \"\\nBeginning Policy Iteration\\n\" + \"-\"*25)\n",
        "\n",
        "V_pi, p_pi = policy_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)\n",
        "render_single(env, p_pi, 150)\n",
        "print(\"\\n\" + \"-\"*25 + \"\\nBeginning Value Iteration\\n\" + \"-\"*25)\n",
        "\n",
        "V_vi, p_vi = value_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)\n",
        "render_single(env, p_vi, 150)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCAt5mf_Oe1B",
        "outputId": "989c767b-e98a-40e7-a31d-9bc0622ba21c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-------------------------\n",
            "Beginning Policy Iteration\n",
            "-------------------------\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Episode reward: 1.000000\n",
            "\n",
            "-------------------------\n",
            "Beginning Value Iteration\n",
            "-------------------------\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FF\u001b[41mF\u001b[0mH\n",
            "HFFG\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Episode reward: 1.000000\n"
          ]
        }
      ]
    }
  ]
}